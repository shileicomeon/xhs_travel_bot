"""
Step 1: ä»å°çº¢ä¹¦æœç´¢çœŸå®å†…å®¹

ä½¿ç”¨MCPå·¥å…·æœç´¢å°çº¢ä¹¦ï¼Œè·å–çœŸå®çš„æ—…æ¸¸å†…å®¹å’Œå›¾ç‰‡
"""

from ..utils.logger import logger
from ..services.xhs_mcp_client import XhsMcpClient, run_async


def search_xhs_content(ctx):
    """
    ä»å°çº¢ä¹¦æœç´¢å†…å®¹
    
    Args:
        ctx: ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«cityã€topic_nameã€topic_typeç­‰ä¿¡æ¯ï¼‰
    
    Returns:
        {
            'feeds': [å¸–å­åˆ—è¡¨],
            'selected_feed': é€‰ä¸­çš„å¸–å­è¯¦æƒ…,
            'images': å›¾ç‰‡URLåˆ—è¡¨
        }
    """
    city = ctx['city']
    topic_name = ctx.get('topic_name', 'æ—…æ¸¸æ”»ç•¥')
    topic_type = ctx.get('topic_type', 'general')
    
    logger.info(f"Step 1: ä»å°çº¢ä¹¦æœç´¢å†…å®¹ - {city} {topic_name} ({topic_type})")
    
    client = XhsMcpClient()
    
    # ğŸ†• æ ¹æ®ä¸»é¢˜ç±»å‹æ„å»ºæœç´¢å…³é”®è¯
    if topic_type == 'landmark':
        # æ™¯ç‚¹ç±»ï¼šå¼ºè°ƒæ”»ç•¥ã€æ‰“å¡ã€æ¸¸ç©
        keywords = [
            f"{city}{topic_name}æ”»ç•¥",
            f"{city}{topic_name}æ¸¸ç©",
            f"{topic_name}æ‰“å¡"
        ]
    elif topic_type == 'food':
        # ç¾é£Ÿç±»ï¼šå¼ºè°ƒæ¨èã€æ¢åº—ã€å¥½åƒ
        keywords = [
            f"{city}{topic_name}æ¨è",
            f"{city}{topic_name}æ¢åº—",
            f"{city}å¥½åƒçš„{topic_name}"
        ]
    elif topic_type == 'drink':
        # é¥®å“ç±»ï¼šå¼ºè°ƒæ¢åº—ã€æ¨èã€å’–å•¡é¦†/èŒ¶é¦†
        keywords = [
            f"{city}{topic_name}æ¢åº—",
            f"{city}{topic_name}æ¨è",
            f"{city}{topic_name}åº—"
        ]
    else:  # general
        # é€šç”¨ç±»ï¼šä¿æŒåŸæœ‰çš„æ—…æ¸¸æ”»ç•¥å…³é”®è¯
        keywords = [
            f"{city}æ—…æ¸¸æ”»ç•¥",
            f"{city}ä¸€æ—¥æ¸¸",
            f"{city}å¿…å»æ™¯ç‚¹"
        ]
    
    all_feeds = []
    
    # æœç´¢å¤šä¸ªå…³é”®è¯
    for keyword in keywords:
        try:
            logger.info(f"æœç´¢: {keyword}")
            feeds = run_async(client.search_feeds(keyword, limit=5))
            all_feeds.extend(feeds)
            
            if len(all_feeds) >= 3:
                break
        
        except Exception as e:
            logger.warning(f"æœç´¢å¤±è´¥: {e}")
            continue
    
    if not all_feeds:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•å†…å®¹")
        raise ValueError("å°çº¢ä¹¦æœç´¢æ— ç»“æœ")
    
    logger.info(f"âœ… å…±æ‰¾åˆ° {len(all_feeds)} ä¸ªç›¸å…³å†…å®¹")
    
    # ç­–ç•¥ï¼šä»å¤šä¸ªå¸–å­æ··åˆæ”¶é›†å›¾ç‰‡ï¼ˆé™ä½é‡å¤ç‡ï¼Œé¿å…ä¾µæƒé£é™©ï¼‰
    all_images = []
    reference_titles = []
    reference_tags = []
    
    logger.info(f"ä» {len(all_feeds)} ä¸ªå¸–å­ä¸­æå–å›¾ç‰‡...")
    
    for feed in all_feeds:
        feed_id = feed.get('feed_id', 'N/A')
        xsec_token = feed.get('xsec_token', '')
        
        try:
            if xsec_token:
                # å°è¯•è·å–è¯¦æƒ…
                detail = run_async(client.get_feed_detail(feed_id, xsec_token))
                
                if detail:
                    images = detail.get('images', [])
                    if images:
                        # ä»æ¯ä¸ªå¸–å­å–éƒ¨åˆ†å›¾ç‰‡ï¼ˆä¸æ˜¯å…¨éƒ¨ï¼‰ï¼Œå¢åŠ å¤šæ ·æ€§
                        take_count = min(len(images), 3)  # æ¯ä¸ªå¸–å­æœ€å¤šå–3å¼ 
                        all_images.extend(images[:take_count])
                        reference_titles.append(detail.get('title', ''))
                        reference_tags.extend(detail.get('tags', []))
                        
                        logger.info(f"  âœ… ä»å¸–å­ {feed_id[:20]}... è·å– {take_count} å¼ å›¾ç‰‡")
                    else:
                        logger.warning(f"  âš ï¸  å¸–å­ {feed_id[:20]}... æ²¡æœ‰å›¾ç‰‡")
            else:
                # æ²¡æœ‰tokenï¼Œè·³è¿‡
                logger.warning(f"  âš ï¸  å¸–å­ {feed_id[:20]}... ç¼ºå°‘xsec_tokenï¼Œè·³è¿‡")
                
        except Exception as e:
            logger.warning(f"  âš ï¸  è·å–å¸–å­ {feed_id[:20]}... å¤±è´¥: {e}")
            continue
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°å›¾ç‰‡ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
    if not all_images:
        error_msg = (
            f"æœªèƒ½ä»å°çº¢ä¹¦è·å–ä»»ä½•å›¾ç‰‡ã€‚\n"
            f"åŸå› ï¼šæ‰€æœ‰ {len(all_feeds)} ä¸ªå¸–å­éƒ½æ— æ³•è·å–å›¾ç‰‡ã€‚\n"
            f"è§£å†³æ–¹æ¡ˆï¼š\n"
            f"  1. æ›´æ–°å°çº¢ä¹¦MCPæœåŠ¡åˆ°æœ€æ–°ç‰ˆæœ¬\n"
            f"  2. ç¡®ä¿MCPæœåŠ¡å·²æ­£ç¡®ç™»å½•\n"
            f"  3. è”ç³»MCPæœåŠ¡æä¾›å•†è§£å†³tokené—®é¢˜"
        )
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    logger.info(f"âœ… å…±è·å– {len(all_images)} å¼ å›¾ç‰‡ï¼ˆæ··åˆè‡ª {len([t for t in reference_titles if t])} ä¸ªå¸–å­ï¼‰")
    
    return {
        'feeds': all_feeds,
        'images': all_images[:10],  # æœ€å¤š10å¼ ï¼Œåç»­ä¼šç­›é€‰åˆ°6å¼ 
        'reference_title': reference_titles[0] if reference_titles else f"{city}æ—…æ¸¸æ”»ç•¥",
        'reference_content': '',  # æ··åˆæ¨¡å¼ä¸‹ä¸ä¿å­˜åŸæ–‡
        'reference_tags': list(set(reference_tags))[:10]  # å»é‡ï¼Œæœ€å¤š10ä¸ª
    }

